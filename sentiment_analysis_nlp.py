# -*- coding: utf-8 -*-
"""Sentiment_Analysis_NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pXu9vfcnzqciEhelw2bshu9PXV1NRUxQ

# üß† Sentiment Analysis on Textual Data
This notebook demonstrates sentiment analysis on tweets/reviews using NLP techniques.
It includes preprocessing, model training, evaluation, and insights.
"""

!pip install seaborn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk

from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

nltk.download('stopwords')
stop_words = stopwords.words('english')

data = {
    'text': [
        "I love this product! It's amazing üòç",
        "Worst experience ever. Totally disappointed.",
        "Meh, it was okay. Nothing special.",
        "Absolutely fantastic service, will buy again!",
        "Terrible! Do not recommend to anyone.",
        "I‚Äôm happy with the results, good quality.",
        "It's not worth the price.",
        "Neutral thoughts on this, still deciding.",
        "Exceeded my expectations!",
        "This is the worst I've used."
    ],
    'sentiment': [
        "positive", "negative", "neutral", "positive", "negative",
        "positive", "negative", "neutral", "positive", "negative"
    ]
}

df = pd.DataFrame(data)
df.head()

def clean_text(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+|https\S+", '', text)  # remove links
    text = re.sub(r'\@w+|\#','', text)  # remove mentions/hashtags
    text = re.sub(r'[^\w\s]', '', text)  # remove punctuation
    text = re.sub(r'\d+', '', text)  # remove numbers
    text = ' '.join(word for word in text.split() if word not in stop_words)  # remove stopwords
    return text

df['clean_text'] = df['text'].apply(clean_text)
df.head()

def clean_text(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+|https\S+", '', text)  # remove links
    text = re.sub(r'\@w+|\#','', text)  # remove mentions/hashtags
    text = re.sub(r'[^\w\s]', '', text)  # remove punctuation
    text = re.sub(r'\d+', '', text)  # remove numbers
    text = ' '.join(word for word in text.split() if word not in stop_words)  # remove stopwords
    return text

df['clean_text'] = df['text'].apply(clean_text)
df.head()

df['label'] = df['sentiment'].map({'positive': 1, 'negative': 0, 'neutral': 2})

X = df['clean_text']
y = df['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

vectorizer = TfidfVectorizer()
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

model = LogisticRegression(multi_class='ovr')
model.fit(X_train_tfidf, y_train)

print("\nClassification Report:\n", classification_report(
    y_test, y_pred, labels=[0, 1, 2], target_names=["negative", "positive", "neutral"]
))

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=["Negative", "Positive", "Neutral"], yticklabels=["Negative", "Positive", "Neutral"])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()